{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IntroToProbabilitySimilarities.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO7edr06iKH4QpQg1P9j+zL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moshebou/ML-Bootcamp/blob/master/IntroToProbabilitySimilarities.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7SYTKVP6qM_",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to probability similarities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyrnVgYuz4wr",
        "colab_type": "text"
      },
      "source": [
        "## Kullback–Leibler divergence (KL-divergence)\n",
        "----\n",
        "Kullback–Leibler divergence (also called relative entropy) is a measure of how one probability distribution is different from a second, reference probability distribution. \\\\\n",
        " \\\\\n",
        "_____\n",
        "### Definition\n",
        "----\n",
        "For discrete probability distributions $P$ and $Q$ defined on the same probability space, $\\chi$, the Kullback–Leibler divergence from $Q$ to $P$ is defined to be:\n",
        "\n",
        "$ {\\displaystyle D_{\\text{KL}}(P\\parallel Q)=\\sum _{x\\in {\\mathcal {X}}}P(x)\\log \\left({\\frac {P(x)}{Q(x)}}\\right).} $ \\\\\n",
        "\n",
        "which is equivalent to:\n",
        "\n",
        "$ {\\displaystyle D_{\\text{KL}}(P\\parallel Q)=-\\sum _{x\\in {\\mathcal {X}}}Q(x)\\log \\left({\\frac {Q(x)}{P(x)}}\\right)}$\n",
        "\n",
        "For distributions ${\\displaystyle P}$ and ${\\displaystyle Q}$ of a continuous random variable, the Kullback–Leibler divergence is defined to be the integral:\n",
        "\n",
        "${\\displaystyle D_{\\text{KL}}(P\\parallel Q)=\\int _{-\\infty }^{\\infty }p(x)\\log \\left({\\frac {p(x)}{q(x)}}\\right)\\,dx}$ \\\\\n",
        "where ${\\displaystyle p}$ and ${\\displaystyle q}$ denote the probability densities of ${\\displaystyle P}$ and ${\\displaystyle Q}$. \\\\\n",
        " \\\\\n",
        "____\n",
        "### Basic examples\n",
        "----\n",
        "1. \\\\\n",
        "Let ${\\displaystyle P}$ and ${\\displaystyle Q}$ be the distributions shown in the table and figure. ${\\displaystyle P}$ is the distribution on the left side of the figure, a binomial distribution with ${\\displaystyle N=2}$ and ${\\displaystyle p=0.4}$. \\\\\n",
        "${\\displaystyle Q}$ is the distribution on the right side of the figure, a discrete uniform distribution with the three possible outcomes ${\\displaystyle x=0}$, ${\\displaystyle 1}$, or ${\\displaystyle 2}$ (i.e. ${\\displaystyle {\\mathcal {X}}=\\{0,1,2\\}}$ ), each with probability ${\\displaystyle p=1/3}$.\n",
        "\n",
        "Two distributions to illustrate Kullback–Leibler divergence\n",
        "\n",
        "x\t| 0 | 1 | 2\n",
        "--- | --- | --- | ---\n",
        "Distribution P(x) | 0.36 | 0.48 | 0.16\n",
        "Distribution Q(x) | 0.333 | 0.333 | 0.333\n",
        "\n",
        "The KL divergences ${\\displaystyle D_{\\text{KL}}(P\\parallel Q)}$ and ${\\displaystyle D_{\\text{KL}}(Q\\parallel P)}$ are calculated as follows. \\\\\n",
        "This example uses the natural log with base e, designated ${\\displaystyle \\operatorname {ln} }$ to get results in nats (see units of information).\n",
        "\n",
        "${\\displaystyle {\\begin{aligned}D_{\\text{KL}}(P\\parallel Q)&=\\sum _{x\\in {\\mathcal {X}}}P(x)\\ln \\left({\\frac {P(x)}{Q(x)}}\\right)\\\\&=0.36\\ln \\left({\\frac {0.36}{0.333}}\\right)+0.48\\ln \\left({\\frac {0.48}{0.333}}\\right)+0.16\\ln \\left({\\frac {0.16}{0.333}}\\right)\\\\&=0.0852996\\end{aligned}}}$\n",
        "\n",
        "\n",
        "${\\displaystyle {\\begin{aligned}D_{\\text{KL}}(Q\\parallel P)&=\\sum _{x\\in {\\mathcal {X}}}Q(x)\\ln \\left({\\frac {Q(x)}{P(x)}}\\right)\\\\&=0.333\\ln \\left({\\frac {0.333}{0.36}}\\right)+0.333\\ln \\left({\\frac {0.333}{0.48}}\\right)+0.333\\ln \\left({\\frac {0.333}{0.16}}\\right)\\\\&=0.097455\\end{aligned}}}$ \\\\\n",
        " \\\\\n",
        "\n",
        "2. \\\\\n",
        "Given the following distribution: \n",
        "\n",
        "x\t| 0 | 1 | 2\n",
        "--- | --- | --- | ---\n",
        "Distribution P(x) | 0 | 1 | 0\n",
        "Distribution Q(x) | 0.333 | 0.333 | 0.333\n",
        "\n",
        "${\\displaystyle {\\begin{aligned}D_{\\text{KL}}(P\\parallel Q)&=\\sum _{x\\in {\\mathcal {X}}}P(x)\\ln \\left({\\frac {P(x)}{Q(x)}}\\right)\\\\&=0\\ln \\left({\\frac {0}{0.333}}\\right)+1\\ln \\left({\\frac {1}{0.333}}\\right)+0\\ln \\left({\\frac {0}{0.333}}\\right)\\\\&=1.0986\\end{aligned}}}$\n",
        "\n",
        "\n",
        "${\\displaystyle {\\begin{aligned}D_{\\text{KL}}(Q\\parallel P)&=\\sum _{x\\in {\\mathcal {X}}}Q(x)\\ln \\left({\\frac {Q(x)}{P(x)}}\\right)\\\\&=0.333\\ln \\left({\\frac {0.333}{0}}\\right)+0\\ln \\left({\\frac {0}{1}}\\right)+0.333\\ln \\left({\\frac {0.333}{0}}\\right)\\\\&= \\infty \\end{aligned}}}$ \\\\\n",
        " \\\\\n",
        "\n",
        "3. \\\\\n",
        "Fig1: The Kullback-Leibler divergence for a normal Gaussian probability distribution. This is an illustration of the area which is integrated when computing the KL divergence. On the top left is an example of two Gaussian PDF’s and to the right of that is the area which when integrated gives the KL metric. Below it are three more examples showing that as the mean between two distributions grows linearly, the area integrated grows much faster.\n",
        "![The Kullback-Leibler divergence for a normal Gaussian probability distribution. This is an illustration of the area which is integrated when computing the KL divergence. On the top left is an example of two Gaussian PDF’s and to the right of that is the area which when integrated gives the KL metric. Below it are three more examples showing that as the mean between two distributions grows linearly, the area integrated grows much faster..](https://upload.wikimedia.org/wikipedia/commons/a/a8/KL-Gauss-Example.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57UERBg67WfU",
        "colab_type": "text"
      },
      "source": [
        "____\n",
        "## Jensen–Shannon divergence\n",
        "----\n",
        "Like the KL-divergence, the Jensen–Shannon divergence is a method of measuring the similarity between two probability distributions. It is also known as information radius (IRad)or total divergence to the average. \\\\\n",
        "It is based on the Kullback–Leibler divergence, with some notable (and useful) differences, including that it is symmetric and it always has a finite value. \n",
        "\n",
        "The square root of the Jensen–Shannon divergence is a metric often referred to as Jensen-Shannon distance.\n",
        "\n",
        "_____\n",
        "### Definition\n",
        "----\n",
        "The Jensen–Shannon divergence (JSD) is a symmetrized and smoothed version of the Kullback–Leibler divergence ${\\displaystyle D(P\\parallel Q)}$. It is defined by\n",
        "\n",
        "$${\\displaystyle {\\rm {JSD}}(P\\parallel Q)={\\frac {1}{2}}D(P\\parallel M)+{\\frac {1}{2}}D(Q\\parallel M)}$$\n",
        "where ${\\displaystyle M={\\frac {1}{2}}(P+Q)}$\n",
        "\n",
        " \\\\\n",
        "____\n",
        "### Basic examples\n",
        "----\n",
        "1.\n",
        "using the same example as before:\n",
        "\n",
        "x\t| 0 | 1 | 2\n",
        "--- | --- | --- | ---\n",
        "Distribution P(x) | 0.36 | 0.48 | 0.16\n",
        "Distribution Q(x) | 0.333 | 0.333 | 0.333\n",
        "Distribution M(x) | 0.3465 | 0.4065 | 0.2465\n",
        "\n",
        "\n",
        "${\\displaystyle\n",
        " {\\rm {JSD}}(P\\parallel Q)=\\\\\n",
        " {\\frac {1}{2}}D(P\\parallel M)+{\\frac {1}{2}}D(Q\\parallel M)=\\\\\n",
        " 0.5\\sum _{x\\in {\\mathcal {X}}}P(x)\\ln \\left({\\frac {P(x)}{M(x)}}\\right) + 0.5\\sum _{x\\in {\\mathcal {X}}}Q(x)\\ln \\left({\\frac {Q(x)}{M(x)}}\\right) = \\\\\n",
        " 0.5\\left( \n",
        "   0.36\\ln \\left({\\frac {0.36}{0.3465}}\\right)+\n",
        "   0.48\\ln \\left({\\frac {0.48}{0.4065}}\\right)+\n",
        "   0.16\\ln \\left({\\frac {0.16}{0.2465}}\\right)+\n",
        "   0.333\\ln \\left({\\frac {0.333}{0.3465}}\\right)+\n",
        "   0.333\\ln \\left({\\frac {0.333}{0.4065}}\\right)+\n",
        "   0.333\\ln \\left({\\frac {0.333}{0.2465}}\\right)\n",
        "   \\right)\\\\\\\\\n",
        "}$\n",
        "\n",
        "\n",
        "\n",
        "2. \n",
        "Given the following distribution: \n",
        "\n",
        "x\t| 0 | 1 | 2\n",
        "--- | --- | --- | ---\n",
        "Distribution P(x) | 0 | 1 | 0\n",
        "Distribution Q(x) | 0.333 | 0.333 | 0.333\n",
        "Distribution M(x) | 0.1667 | 0.6665 | 0.1667\n",
        "\n",
        "${\\displaystyle\n",
        " {\\rm {JSD}}(P\\parallel Q)=\\\\\n",
        " {\\frac {1}{2}}D(P\\parallel M)+{\\frac {1}{2}}D(Q\\parallel M)=\\\\\n",
        " 0.5\\sum _{x\\in {\\mathcal {X}}}P(x)\\ln \\left({\\frac {P(x)}{M(x)}}\\right) + 0.5\\sum _{x\\in {\\mathcal {X}}}Q(x)\\ln \\left({\\frac {Q(x)}{M(x)}}\\right) = \\\\\n",
        " 0.5\\left( \n",
        "   0\\ln \\left({\\frac {0}{0.1667}}\\right)+\n",
        "   1\\ln \\left({\\frac {1}{0.6665}}\\right)+\n",
        "   0\\ln \\left({\\frac {0}{0.1667}}\\right)+\n",
        "   0.333\\ln \\left({\\frac {0.333}{0.1667}}\\right)+\n",
        "   0.333\\ln \\left({\\frac {0.333}{0.6665}}\\right)+\n",
        "   0.333\\ln \\left({\\frac {0.333}{0.1667}}\\right)\n",
        "   \\right)\\\\\\\\\n",
        "}$\n",
        "\n",
        "____\n",
        "### Comparison between KL divergence and JS divergence\n",
        "----\n",
        "Given two Gaussian distribution, p with mean=0 and std=1 and q with mean=1 and std=1. The average of two distributions is labelled as m=(p+q)/2. KL divergence DKL is asymmetric but JS divergence DJS is symmetric.\n",
        "![Given two Gaussian distribution, p with mean=0 and std=1 and q with mean=1 and std=1. The average of two distributions is labelled as m=(p+q)/2. KL divergence DKL is asymmetric but JS divergence DJS is symmetric.](https://lilianweng.github.io/lil-log/assets/images/KL_JS_divergence.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAmobKfpegtN",
        "colab_type": "text"
      },
      "source": [
        "____\n",
        "## Wasserstein metric\n",
        "----\n",
        "In mathematics, the Wasserstein or Kantorovich–Rubinstein metric or distance is a distance function defined between probability distributions on a given metric space $M$.\n",
        "\n",
        "Intuitively, if each distribution is viewed as a unit amount of \"dirt\" piled on $M$, the metric is the minimum \"cost\" of turning one pile into the other, which is assumed to be the amount of dirt that needs to be moved times the mean distance it has to be moved. Because of this analogy, the metric is known in computer science as the earth mover's distance.\n",
        "\n",
        "_____\n",
        "### Definition\n",
        "----\n",
        "Let $(M,d)$ be a metric space for which every probability measure on $M$ is a Radon measure (a so-called Radon space). For $p\\geq 1$, let $\\displaystyle P_{p}(M)$ denote the collection of all probability measures $\\mu$  on $M$ with finite $p^{\\text{th}}$ moment. Then, there exists some $x_{0}$ in $M$ such that:\n",
        "\n",
        "$$ \\\\\n",
        "{\\displaystyle \\int _{M}d(x,x_{0})^{p}\\,\\mathrm {d} \\mu (x)<+\\infty .}\n",
        "$$\n",
        "\n",
        "The $p^{\\text{th}}$ Wasserstein distance between two probability measures $\\mu$  and $\\nu$  in $P_{p}(M)$ is defined as:\n",
        "\n",
        "$$\n",
        "{\\displaystyle W_{p}(\\mu ,\\nu ):=\\left(\\inf _{\\gamma \\in \\Gamma (\\mu ,\\nu )}\\int _{M\\times M}d(x,y)^{p}\\,\\mathrm {d} \\gamma (x,y)\\right)^{1/p},}\n",
        "$$\n",
        "\n",
        "where $\\Gamma (\\mu ,\\nu )$ denotes the collection of all measures on $M\\times M$ with marginals $\\mu$  and $\\nu$  on the first and second factors respectively. (The set $\\Gamma (\\mu ,\\nu )$ is also called the set of all couplings of $\\mu$  and $\\nu$ .)\n",
        "\n",
        "The above distance is usually denoted $W_{p}(\\mu ,\\nu )$ (typically among authors who prefer the \"Wasserstein\" spelling) or $\\ell _{p}(\\mu ,\\nu )$ (typically among authors who prefer the \"Vaserstein\" spelling). The remainder of this article will use the $W_p$ notation.\n",
        "\n",
        "The Wasserstein metric may be equivalently defined by\n",
        "$$\n",
        "{\\displaystyle W_{p}(\\mu ,\\nu )=\\left(\\inf \\operatorname {E} {\\big [}d(X,Y)^{p}{\\big ]}\\right)^{1/p},}\n",
        "$$\n",
        "\n",
        "where $\\mathbf {E} [Z]$ denotes the expected value of a random variable $Z$ and the infimum is taken over all joint distributions of the random variables $X$ and $Y$ with marginals $\\mu$  and $\\nu$  respectively.\n",
        " \\\\\n",
        "____\n",
        "### Basic examples\n",
        "----\n",
        "\n",
        "Suppose we have two distributions $P$ and $Q$, each has four piles of dirt and both have ten shovelfuls of dirt in total. The numbers of shovelfuls in each dirt pile are assigned as follows:\n",
        "\n",
        "$P1=3,P2=2,P3=1,P4=4 \\\\ \n",
        "Q1=1,Q2=2,Q3=4,Q4=3$ \\\\\n",
        "In order to change $P$ to look like $Q$, as illustrated in Fig.3, we:\n",
        "\n",
        "First move 2 shovelfuls from $P_1$ to $P_2 => ($P_1$, $Q_1$)$ match up. \\\n",
        "Then move 2 shovelfuls from $P_2$ to $P_3 => (P_2,Q_2)$ match up. \\\n",
        "Finally move 1 shovelfuls from $Q_3$ to $Q_4 => (P_3,Q_3)$ and $(P_4, Q_4)$ match up.\n",
        "\n",
        "If we label the cost to pay to make $P_i$ and $Q_i$ match as $\\delta_i$, we would have $\\delta_{i+1}=\\delta_i+P_i−Q_i$ and in the example:\n",
        "\n",
        "$\\delta_0=0 \\\\\n",
        "\\delta_1=0+3−1=2 \\\\\n",
        "\\delta_2=2+2-2=2 \\\\ \n",
        "\\delta_3=2+1-4=-1 \\\\ \n",
        "\\delta_4=-1+4-3=0$\n",
        "\n",
        "Finally the Earth Mover’s distance is $W=∑|\\delta_i|=5$.\n",
        "\n",
        "\n",
        "\n",
        "Fig.3 Step-by-step plan of moving dirt between piles in P and Q to make them match.\n",
        "![Step-by-step plan of moving dirt between piles in P and Q to make them match.](https://lilianweng.github.io/lil-log/assets/images/EM_distance_discrete.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9Wup8uA7UVh",
        "colab_type": "text"
      },
      "source": [
        " ____\n",
        "## Reference\n",
        "----\n",
        "* https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n",
        "* https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html\n",
        "* https://en.wikipedia.org/wiki/Wasserstein_metric\n",
        "* https://medium.com/@jonathan_hui/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490\n"
      ]
    }
  ]
}